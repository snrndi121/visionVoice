#1. 서두
#1. 영상으로 사람이 무슨 말을 하고 있는 지 걸러낼 수 있을 까?
일반적으로, 인간은 입모양을 보면서 정확히는 아니더라도 어떤 말을 하고 있는 지 추론을 할 수 있다. 가장 유사하게 발음이 나는 것들을 고려를 하고 현재 상황에 대조를 해보면서 그 의미를 이해하려고 한다. 흡사 귀를 막고 상대방의 입만 보고 주어진 단어에 대한 설명을 주고 받으며 문제를 맞추는 게임과 같다. 이러한 게임에서
사람 또한 굉장히 많은 시행 착오를 겪는 것을 확인할 수 있으며 상대방의 반응을 통해서 점차 정답에 가까워지는 광경을 볼 수 있다. 입모양으로 도출될 수 있는 경우의 수가 많기 때문에 그 집합의 범위를 줄연나가기 때문에 도달할 수 있는 것이다.

#1.2 이러한 기능을 구현하기 위하여 고려한 것은 타이밍과 이미지 분류이다.
영상을 통해서 인식되는 사람으로부터 정확히 언제 입을 열고 닫는 지만 안다면 STT로 들어온 전체 대화문에서 골라내는 작업으로 어느 정도 가능할 것이라고 생각하였다. 다만, 100%에 가까운 성능을 내기 위해서는 조금 더 생각할 요소들은 있을 것이다. 1/60 frame일 때, 그 순간을 캐치하고 정하는 것은 신중할 수 있다. 그리고 기본적으로, 발음을 할 때 사람들이 보이는 입 모양은 패턴은 유사하나 결국에는 다르다는 점이다. 두 번째는, 영상을 통해서 현재 입모양에 대한 이미지 분류 작업을 하여 이것을 통하여 무슨 단어과 문장들을 생성하는 지 조합을 만들어 내고 예측하는 방법이다. 이를 위해서는, 두 가지 지도 학습이 선행되어야 한다. 먼저 이미지와 입모양 간에 지도 학습, 입모양 패턴과 가능한 단어/문장 집합에 대한 학습이다.

#1.3 상황에 대한 구체화를 통하여 필요한 파트별 세부 구현 상황을 고려해보기로 한다.
#상황1) 카메라 앞에 화자는 1명 + 말하는 사람이 정확히 교대로 말하며 + STT가 동작하는 상황.
이 경우는, 말하는 타이밍을 정확히 잡아낸다면 이미지 분류까지의 상황으로 연결되지 않을 수 있다.
#상황2) 카메라 앞에 화자는 1명 + 말하는 사람이 섞여서 말하는 상황 + STT가 동작하는 상황
이 경우는, 타이밍만으로 해결하기 어려운 상황이다. 예를 들어, 발화자가 말했던 0~10초 동안 카메라를 통하여 정면의 발화자의 입 타이밍을 캐치하여서 주어진 문장에서 슬라이스 작업을 할 것이다. 앞 전의 상황과 다른 점은, 섞여서 말하는 상황이기 때문에 온전히 발화자만의 발화가 들어간 상황이 아니기 때문에 골라내는 작업들이 필요하다. 이미지 분류를 통하여 주어진 문장에서 '골라내는 작업'이 필요하다.
#상황3) 상황1, 2에서 카메라 앞에 화자가 N명이 되는 경우,
카메라를 통하여 얼굴 인식이 되는 문제라면 각 발화자의 이미지에 대한 정보를 따로 저장하면 되는 부분이므로 큰 문제가 되지 않을 듯 하지만, 이 경우는 본 과제에서 빼기로 한다.

#상황4) 카메라 앞에 화자는 1명 + 말하는 사람이 정확히 교대로 말하며/말하는 사람이 섞여서 말하는 상황 + STT가 동작하지 않는 상황
STT가 동작하지 않는 상황이기 때문에 전체 대화문에 대한 정보가 생성되지 않기 때문에 상황(4-1)과 같이 조건이 축소가 된다.
#상황4-1) 카메라 앞에 화자는 1명 + STT가 동작하지 않는 상황
슬라이스를 할 전체 대화문이 없기 때문에 타이밍 방식은 동작하지 않는다. 따라서, 입모양으로 화자가 어떠한 대화문을 생성하고 있는 지 예측을 해야 한다.

#결과적으로 상황은 다음과 같이 압축이 된다. 카메라 앞에 몇 명이 서있는 지가 아니라, *STT를 사용하는지 안 하는 지에 대하여* 솔루션을 만들어내야 한다.

#2.진행 방향
최종적으로는 STT 없는, '귀가 먼 상태'에서 동작할 수 있어야 한다. STT를 사용하는 경우는 발화자에 대한 초기 세팅을 하는 과정에서 역치값(threshold)을 설정하는 단계이다. 즉, 입이 열리고 닫히는 타이밍을 보다 정확히 맞추기 위한 때이다. 특정한 문장을 발화자에게 읽도록 지시를 하여 @의미있는 변수를 만들어내는 작업이다. 그리고, STT가 없는 상황에서 앞선 바이어스 해놓은 설정을 토대로 이미지 분류에 활용하는 것이다. 그러나, 이 단계는 입이 열림 정도를 판단하는 정도에 그칠 것이고 실질적인 분류 작업은 사전에 만들어낸 모델을 바탕으로 수행을 해낼 것이다.

#2.1 각 사용자에 대한 논의
사용자에 대하여 Single/Group(Mutiple)로 구분지을 수 있다. 이것은 현단계의 얼굴 인식에서 원활하게 가능한 부분이고, 다만 Mutiple을 수행하게 되었을 때 각 발화자 객체를 생성하여 인식된 부분을 넣어줘야할 것이다. 그런 다음에, 해당 그룹에서 누구를 찾는 지 '식별자'를 찾아낼 수 있어야한다. 그러나, 최종적으로 활용될 방안을 고려하였을 때 '주요 발화자'와 대화를 하는 와중에 인식된 음성들 속에서 해당 발화자 이외에 필터링을 할 목적이기 때문에 모두 저장할 필요는 없게 된다. 따라서, 시나리오를 만들어보게 된다면 화면 속에서 어떤 화자의 음성을 분류해낼 지 미리 선택하여 화자 분리를 시작하거나 또는 화면 상에서 제일 크게 잡히는 얼굴을 주요 발화자로 자동으로 인식하고 화자 분리를 해야할 것이다. 그렇다면 나오게 되는 옵션의 이름을 가명 짓자면, *'선택적 분리'와 '기초적 분리'라는 용어를 생각해볼 수 있다.*

#2.2 음성 인식과의 연계 논의
영상 인식과 함께 음성 인식을 연계한다면 보다 성능을 높일 수 있을 것이라는 예측을 하였다. 그랬을 때, 음성이 들어오는 상황에 대하여 2가지로 분류 지었다. 첫번째는, '교차적 음성'으로 음성 그래프상 중첩의 상황이 발생하지 않는 상황으로 일종의 1:1의 상황이다. 그렇기 때문에 특정 타이밍 구간에서 인식되는 발화가 단일 화자이기 때문에 영상 인식과 결합을 할 때 영상은 입의 개폐 유무만 정확히 찾아내는 용도 이상으로는 사용되지 않을 것이다. 두 번째는 '중첩적 음성'으로 같은 타이밍 구간에서 여러 개의 발화가 진행되어 음성 인식이후 문자열로 변환하는 과정에서 여러 개의 말들이 뭉쳐서 나타나게 될 것이다. 이 때 이미지 분류의 결과값으로 걸러내는 용도로 사용해볼 수 있다. 이런 2 가지 분류를 생각은 해볼 수 있었지만 그렇다면 실제로 중첩되는 음성이 있을 때 단순히 텍스트만으로 걸러내는 게 논리적인지 고려한다면 '그렇지 않다'이다. 이 부분은 오히려 음성 파형을 통한 화자 분리가 더 합리적인 결과를 도출해낼 것이다. *따라서, 본 프로젝트를 진행함에 있어서 '교차적 음성'의 상황만을 고려하고 '중첩적 음성'은 음성 파형을 통한 과제로 넘기는 편이 맞다고 생각했다.*

#2.2 이미지 분류에 대한 논의
음성 인식과 연계를 하는 과정에서 교차적 음성만 고려한다면 이미지 분류 작업은 필요하지 않다. 입이 벌어졌다고 판단할 수 있는 역치값만 특정 문장을 읽게함으로써 값 하나만 뽑아내면 된다. 이미지 분류가 필요한 순간은 음성 인식이 들어가지 않는 시점이고 위에서 언급하였던 @귀가 먼 상태에서 입모양의 모음을 따와서 이후 이 집합들을 가지고 생성 가능한 문장들을 출력해낼 수 있다면 본 프로젝트가 완료되는 시점이다. 이를 위해서는 이미지에서 얼굴 -> 입술 -> 분류 작업이 일련의 작업이 되어서 제대로 이뤄져야 하며 사전에 이미지 분류 모델을 위한 데이터 셋을 올바르게 만들어 내야한다.

#2.4 모음 집합에서 생성 가능한 문장 만들기
가장 핵심 알고리즘이 될 부분으로, 이미지 분류로 생성된 단일 문자열을 가지고 조합 가능하며 그럴듯한 문자열을 만들어 내야한다. 
#2.5 데이터와 전처리 작업



#3.설계
#인식-분류-치환-조합-추출
#4.구현
@part1_인식
@part2_분류
@part3_치환
@part4_조합
@part5_추출
#5.결과
